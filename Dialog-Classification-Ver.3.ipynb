{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os, sys\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sent):\n",
    "    sent = sent.strip()\n",
    "    sent = re.sub('{laughing}|{clearing}|{singing}|{applauding}', '', sent)\n",
    "    sent = re.sub('[(][(].*?[)][)]|-.*?-', '', sent)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_main_topic = lambda x: x.split(' > ')[0]\n",
    "\n",
    "def load_data():\n",
    "    file_list = [f_name for f_name in os.listdir('data') if f_name[-5:] == '.json']\n",
    "    \n",
    "    total_data = []\n",
    "    for f_name in tqdm(file_list):\n",
    "        with open('data/%s' %f_name, 'r') as f:\n",
    "            data = json.loads(f.read())['document'][0]\n",
    "            metadata = data['metadata']\n",
    "            utterance = data['utterance']\n",
    "            \n",
    "            topic = get_main_topic(metadata['topic'])\n",
    "            if topic[:4] == 'NWRW':\n",
    "                continue\n",
    "\n",
    "            last_speaker = None\n",
    "            seg1 = seg2 = ''\n",
    "            for u in utterance:\n",
    "                if last_speaker is None:\n",
    "                    last_speaker = u['speaker_id']\n",
    "                    seg2 = u['form']\n",
    "                elif last_speaker == u['speaker_id']:\n",
    "                    seg2 += ' ' + u['original_form']\n",
    "                else:\n",
    "                    if seg1 and seg2:\n",
    "                        total_data.append([f_name, topic, clean_sentence(seg1), clean_sentence(seg2)])\n",
    "                    last_speaker = u['speaker_id']\n",
    "                    seg1 = seg2\n",
    "                    seg2 = u['original_form']\n",
    "            if seg1 and seg2:\n",
    "                total_data.append([f_name, topic, clean_sentence(seg1), clean_sentence(seg2)])\n",
    "                \n",
    "    return np.array(total_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n",
      "using cached model\n",
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "bertmodel, vocab = get_pytorch_kobert_model()\n",
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting parameters\n",
    "pre_max_len = 4096\n",
    "max_len = 512\n",
    "batch_size = 6\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 25\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate =  5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.indices([pre_max_len])[0]\n",
    "def pad_transform(transform, sent1, sent2, max_len):\n",
    "    tokens, valid_len, segs = transform([sent1, sent2])\n",
    "    if valid_len < max_len:\n",
    "        return tokens[:max_len], np.array((tokens != 1).sum(), dtype='int32'), segs[:max_len]\n",
    "    \n",
    "    idx2 = indices[segs == 1][:-1]\n",
    "    idx1 = indices[:idx2[0]][1:-1]\n",
    "    if len(idx1) < len(idx2) and (len(idx1) * 2 + 3) < max_len:\n",
    "        idx2 = idx2[:(max_len - 3 - len(idx1))]\n",
    "    elif len(idx2) < len(idx1) and (len(idx2) * 2 + 3) < max_len:\n",
    "        idx1 = idx1[-(max_len - 3 - len(idx2)):]\n",
    "    else:\n",
    "        if len(idx1) < len(idx2):\n",
    "            idx1, idx2 = idx1[-((max_len - 3) // 2):], idx2[:((max_len - 3) // 2 + 1)]\n",
    "        else:\n",
    "            idx1, idx2 = idx1[-((max_len - 3) // 2 + 1):], idx2[:((max_len - 3) // 2)]\n",
    "    return (np.concatenate([[2], tokens[idx1], [3], tokens[idx2],[3]]), \n",
    "             np.array(max_len, dtype='int32'), \n",
    "             np.array([0] * (len(idx1) + 2) + [1] * (len(idx2) + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx1, sent_idx2, label_idx, bert_tokenizer, max_len,\n",
    "                 pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=pre_max_len, pad=pad, pair=pair)\n",
    "\n",
    "        self.sentences = [pad_transform(transform, d[sent_idx1], d[sent_idx2], max_len) for d in tqdm(dataset)]\n",
    "#         self.sentences = [transform([d[sent_idx1], d[sent_idx2]]) for d in tqdm(dataset)]\n",
    "        self.labels = [np.int32(d[label_idx]) for d in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=2,\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        ret = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        pooler = ret[1]\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_to_list(row):\n",
    "    return str(row[2]), str(row[3]), label_w2i[row[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2232/2232 [00:02<00:00, 1062.10it/s]\n"
     ]
    }
   ],
   "source": [
    "raw_dataset = load_data()\n",
    "file_with_label = np.unique(raw_dataset[:, :2], axis=0)\n",
    "_labels, _counts = np.unique(file_with_label[:, 1], return_counts=True)\n",
    "label_counts = dict(zip(_labels, _counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "np.random.shuffle(file_with_label)\n",
    "\n",
    "valid_list = {label: [] for label in label_counts}\n",
    "test_list = {label: [] for label in label_counts}\n",
    "\n",
    "for file in file_with_label:\n",
    "    if len(valid_list[file[1]]) < label_counts[file[1]] * 0.1:\n",
    "        valid_list[file[1]].append(file[0])\n",
    "    elif len(test_list[file[1]]) < label_counts[file[1]] * 0.1:\n",
    "        test_list[file[1]].append(file[0])\n",
    "        \n",
    "_valid_list = []\n",
    "_test_list = []\n",
    "for label in label_counts:\n",
    "    _valid_list.extend(valid_list[label])\n",
    "    _test_list.extend(test_list[label])\n",
    "    \n",
    "valid_list = _valid_list\n",
    "test_list = _test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_dataset, raw_valid_dataset, raw_test_dataset = [], [], []\n",
    "for row in raw_dataset:\n",
    "    if row[0] in valid_list:\n",
    "        raw_valid_dataset.append(row)\n",
    "    elif row[0] in test_list:\n",
    "        raw_test_dataset.append(row)\n",
    "    else:\n",
    "        raw_train_dataset.append(row)\n",
    "raw_train_dataset, raw_valid_dataset, raw_test_dataset = np.array(raw_train_dataset), np.array(raw_valid_dataset), np.array(raw_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('raw_train_valid_test.npz', raw_train_dataset, raw_valid_dataset, raw_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "npz_file = np.load('raw_train_valid_test.npz')\n",
    "raw_train_dataset, raw_valid_dataset, raw_test_dataset = npz_file['arr_0'], npz_file['arr_1'], npz_file['arr_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_i2w = np.unique(raw_train_dataset[:, 1]).tolist()\n",
    "label_w2i = {w: i for i, w in enumerate(label_i2w)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_w2i = pickle.load(open('ver_3.w2i', 'rb'))\n",
    "label_i2w = {label_w2i[l]: l for l in label_w2i}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset = [row_to_list(r) for r in raw_train_dataset]\n",
    "valid_dataset = [row_to_list(r) for r in raw_valid_dataset]\n",
    "test_dataset = [row_to_list(r) for r in raw_test_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18629/18629 [00:35<00:00, 517.62it/s]\n",
      "100%|██████████| 2451/2451 [00:04<00:00, 535.81it/s]\n",
      "100%|██████████| 2277/2277 [00:04<00:00, 483.96it/s]\n"
     ]
    }
   ],
   "source": [
    "data_train = BERTDataset(train_dataset, 0, 1, 2, tok, max_len, True, True)\n",
    "data_valid = BERTDataset(valid_dataset, 0, 1, 2, tok, max_len, True, True)\n",
    "data_test = BERTDataset(test_dataset, 0, 1, 2, tok, max_len, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)\n",
    "valid_dataloader = torch.utils.data.DataLoader(data_valid, batch_size=batch_size, num_workers=5)\n",
    "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERTClassifier(bertmodel, dr_rate=0.5, num_classes=len(label_i2w)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "# loss_fn = nn.CrossEntropyLoss(weight=weight)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 01 batch id 0001/3105 loss 3.090259 train acc 0.000000\n",
      "epoch 01 batch id 0201/3105 loss 2.972554 train acc 0.053897\n",
      "epoch 01 batch id 0401/3105 loss 2.639045 train acc 0.068579\n",
      "epoch 01 batch id 0601/3105 loss 2.756540 train acc 0.105380\n",
      "epoch 01 batch id 0801/3105 loss 2.507110 train acc 0.153142\n",
      "epoch 01 batch id 1001/3105 loss 1.937168 train acc 0.240926\n",
      "epoch 01 batch id 1201/3105 loss 0.536893 train acc 0.307799\n",
      "epoch 01 batch id 1401/3105 loss 0.522561 train acc 0.375922\n",
      "epoch 01 batch id 1601/3105 loss 1.553600 train acc 0.427753\n",
      "epoch 01 batch id 1801/3105 loss 2.545682 train acc 0.467426\n",
      "epoch 01 batch id 2001/3105 loss 1.301041 train acc 0.487923\n",
      "epoch 01 batch id 2201/3105 loss 0.614586 train acc 0.510147\n",
      "epoch 01 batch id 2401/3105 loss 0.292521 train acc 0.535402\n",
      "epoch 01 batch id 2601/3105 loss 0.180351 train acc 0.558183\n",
      "epoch 01 batch id 2801/3105 loss 0.013120 train acc 0.578365\n",
      "epoch 01 batch id 3001/3105 loss 0.300234 train acc 0.588359\n",
      "100%|██████████| 3105/3105 [12:16<00:00,  4.22it/s]\n",
      "epoch 01 train acc 0.595008\n",
      "100%|██████████| 409/409 [00:32<00:00, 12.73it/s]\n",
      "epoch 01 valid acc 0.815811\n",
      "epoch 02 batch id 0001/3105 loss 0.024314 train acc 1.000000\n",
      "epoch 02 batch id 0201/3105 loss 0.722308 train acc 0.713930\n",
      "epoch 02 batch id 0401/3105 loss 0.045286 train acc 0.762261\n",
      "epoch 02 batch id 0601/3105 loss 0.941203 train acc 0.787022\n",
      "epoch 02 batch id 0801/3105 loss 0.008445 train acc 0.798793\n",
      "epoch 02 batch id 1001/3105 loss 0.004580 train acc 0.800366\n",
      "epoch 02 batch id 1201/3105 loss 0.011841 train acc 0.795448\n",
      "epoch 02 batch id 1401/3105 loss 0.017341 train acc 0.799667\n",
      "epoch 02 batch id 1601/3105 loss 0.175992 train acc 0.804601\n",
      "epoch 02 batch id 1801/3105 loss 2.124979 train acc 0.803813\n",
      "epoch 02 batch id 2001/3105 loss 0.006285 train acc 0.792687\n",
      "epoch 02 batch id 2201/3105 loss 0.172857 train acc 0.787142\n",
      "epoch 02 batch id 2401/3105 loss 0.025681 train acc 0.781896\n",
      "epoch 02 batch id 2601/3105 loss 0.036133 train acc 0.780725\n",
      "epoch 02 batch id 2801/3105 loss 0.004023 train acc 0.785315\n",
      "epoch 02 batch id 3001/3105 loss 0.250604 train acc 0.782295\n",
      "100%|██████████| 3105/3105 [12:18<00:00,  4.21it/s]\n",
      "epoch 02 train acc 0.783468\n",
      "100%|██████████| 409/409 [00:32<00:00, 12.77it/s]\n",
      "epoch 02 valid acc 0.820293\n",
      "epoch 03 batch id 0001/3105 loss 0.272099 train acc 0.833333\n",
      "epoch 03 batch id 0201/3105 loss 0.031188 train acc 0.723051\n",
      "epoch 03 batch id 0401/3105 loss 0.006347 train acc 0.758520\n",
      "epoch 03 batch id 0601/3105 loss 3.841763 train acc 0.772879\n",
      "epoch 03 batch id 0801/3105 loss 0.003290 train acc 0.790054\n",
      "epoch 03 batch id 1001/3105 loss 0.003159 train acc 0.797203\n",
      "epoch 03 batch id 1201/3105 loss 0.007002 train acc 0.798085\n",
      "epoch 03 batch id 1401/3105 loss 0.006825 train acc 0.798001\n",
      "epoch 03 batch id 1601/3105 loss 1.312472 train acc 0.803873\n",
      "epoch 03 batch id 1801/3105 loss 2.758300 train acc 0.802517\n",
      "epoch 03 batch id 2001/3105 loss 0.931433 train acc 0.786523\n",
      "epoch 03 batch id 2201/3105 loss 0.537025 train acc 0.781539\n",
      "epoch 03 batch id 2401/3105 loss 0.002506 train acc 0.785020\n",
      "epoch 03 batch id 2601/3105 loss 0.015449 train acc 0.790209\n",
      "epoch 03 batch id 2801/3105 loss 0.001703 train acc 0.796025\n",
      "epoch 03 batch id 3001/3105 loss 0.009028 train acc 0.792847\n",
      "100%|██████████| 3105/3105 [12:17<00:00,  4.21it/s]\n",
      "epoch 03 train acc 0.794632\n",
      "100%|██████████| 409/409 [00:31<00:00, 12.86it/s]\n",
      "epoch 03 valid acc 0.822331\n",
      "epoch 04 batch id 0001/3105 loss 0.011119 train acc 1.000000\n",
      "epoch 04 batch id 0201/3105 loss 0.043138 train acc 0.787728\n",
      "epoch 04 batch id 0401/3105 loss 0.003592 train acc 0.820865\n",
      "epoch 04 batch id 0601/3105 loss 2.206787 train acc 0.825846\n",
      "epoch 04 batch id 0801/3105 loss 0.008452 train acc 0.831253\n",
      "epoch 04 batch id 1001/3105 loss 5.818701 train acc 0.829837\n",
      "epoch 04 batch id 1201/3105 loss 0.005410 train acc 0.827366\n",
      "epoch 04 batch id 1401/3105 loss 0.006277 train acc 0.831430\n",
      "epoch 04 batch id 1601/3105 loss 0.013699 train acc 0.836248\n",
      "epoch 04 batch id 1801/3105 loss 2.693243 train acc 0.835462\n",
      "epoch 04 batch id 2001/3105 loss 0.003531 train acc 0.826337\n",
      "epoch 04 batch id 2201/3105 loss 0.009188 train acc 0.825004\n",
      "epoch 04 batch id 2401/3105 loss 0.360550 train acc 0.826669\n",
      "epoch 04 batch id 2601/3105 loss 0.923839 train acc 0.828720\n",
      "epoch 04 batch id 2801/3105 loss 0.002245 train acc 0.832679\n",
      "epoch 04 batch id 3001/3105 loss 0.037560 train acc 0.830945\n",
      "100%|██████████| 3105/3105 [12:13<00:00,  4.23it/s]\n",
      "epoch 04 train acc 0.832260\n",
      "100%|██████████| 409/409 [00:31<00:00, 12.81it/s]\n",
      "epoch 04 valid acc 0.771394\n",
      "epoch 05 batch id 0001/3105 loss 0.005665 train acc 1.000000\n",
      "epoch 05 batch id 0201/3105 loss 0.010608 train acc 0.805970\n",
      "epoch 05 batch id 0401/3105 loss 0.003291 train acc 0.837074\n",
      "epoch 05 batch id 0601/3105 loss 1.187904 train acc 0.839989\n",
      "epoch 05 batch id 0801/3105 loss 0.004634 train acc 0.846442\n",
      "epoch 05 batch id 1001/3105 loss 0.003706 train acc 0.847486\n",
      "epoch 05 batch id 1201/3105 loss 0.004729 train acc 0.847766\n",
      "epoch 05 batch id 1401/3105 loss 0.005521 train acc 0.851178\n",
      "epoch 05 batch id 1601/3105 loss 0.856617 train acc 0.854882\n",
      "epoch 05 batch id 1801/3105 loss 1.787926 train acc 0.855081\n",
      "epoch 05 batch id 2001/3105 loss 0.006728 train acc 0.849742\n",
      "epoch 05 batch id 2201/3105 loss 1.042833 train acc 0.846964\n",
      "epoch 05 batch id 2401/3105 loss 1.955456 train acc 0.845550\n",
      "epoch 05 batch id 2601/3105 loss 0.008855 train acc 0.845508\n",
      "epoch 05 batch id 2801/3105 loss 0.002112 train acc 0.847138\n",
      "epoch 05 batch id 3001/3105 loss 0.123960 train acc 0.839942\n",
      "100%|██████████| 3105/3105 [12:16<00:00,  4.22it/s]\n",
      "epoch 05 train acc 0.839184\n",
      "100%|██████████| 409/409 [00:31<00:00, 12.79it/s]\n",
      "epoch 05 valid acc 0.774246\n",
      "epoch 06 batch id 0001/3105 loss 0.004660 train acc 1.000000\n",
      "epoch 06 batch id 0201/3105 loss 0.013766 train acc 0.763682\n",
      "epoch 06 batch id 0401/3105 loss 0.016962 train acc 0.795511\n",
      "epoch 06 batch id 0601/3105 loss 3.785051 train acc 0.802551\n",
      "epoch 06 batch id 0801/3105 loss 0.005789 train acc 0.812734\n",
      "epoch 06 batch id 1001/3105 loss 0.002212 train acc 0.822178\n",
      "epoch 06 batch id 1201/3105 loss 0.003745 train acc 0.826395\n",
      "epoch 06 batch id 1401/3105 loss 0.005741 train acc 0.830954\n",
      "epoch 06 batch id 1601/3105 loss 1.258344 train acc 0.830314\n",
      "epoch 06 batch id 1801/3105 loss 3.755852 train acc 0.828244\n",
      "epoch 06 batch id 2001/3105 loss 0.003713 train acc 0.814093\n",
      "epoch 06 batch id 2201/3105 loss 0.211678 train acc 0.811222\n",
      "epoch 06 batch id 2401/3105 loss 0.087056 train acc 0.807303\n",
      "epoch 06 batch id 2601/3105 loss 0.008071 train acc 0.813085\n",
      "epoch 06 batch id 2801/3105 loss 0.003516 train acc 0.816911\n",
      "epoch 06 batch id 3001/3105 loss 0.026764 train acc 0.814895\n",
      "100%|██████████| 3105/3105 [12:17<00:00,  4.21it/s]\n",
      "epoch 06 train acc 0.816425\n",
      "100%|██████████| 409/409 [00:32<00:00, 12.77it/s]\n",
      "epoch 06 valid acc 0.799919\n",
      "epoch 07 batch id 0001/3105 loss 0.009015 train acc 1.000000\n",
      "epoch 07 batch id 0201/3105 loss 0.241016 train acc 0.807629\n",
      "epoch 07 batch id 0401/3105 loss 0.005230 train acc 0.823774\n",
      "epoch 07 batch id 0601/3105 loss 0.114285 train acc 0.833888\n",
      "epoch 07 batch id 0801/3105 loss 0.009524 train acc 0.849147\n",
      "epoch 07 batch id 1001/3105 loss 2.403581 train acc 0.852647\n",
      "epoch 07 batch id 1201/3105 loss 0.007247 train acc 0.853039\n",
      "epoch 07 batch id 1401/3105 loss 0.003648 train acc 0.854628\n",
      "epoch 07 batch id 1601/3105 loss 0.026256 train acc 0.848740\n",
      "epoch 07 batch id 1801/3105 loss 2.403090 train acc 0.845086\n",
      "epoch 07 batch id 2001/3105 loss 0.003687 train acc 0.836415\n",
      "epoch 07 batch id 2201/3105 loss 0.013172 train acc 0.837877\n",
      "epoch 07 batch id 2401/3105 loss 0.007990 train acc 0.840414\n",
      "epoch 07 batch id 2601/3105 loss 0.020965 train acc 0.844098\n",
      "epoch 07 batch id 2801/3105 loss 0.002828 train acc 0.848090\n",
      "epoch 07 batch id 3001/3105 loss 0.034289 train acc 0.848717\n",
      "100%|██████████| 3105/3105 [12:19<00:00,  4.20it/s]\n",
      "epoch 07 train acc 0.849007\n",
      "100%|██████████| 409/409 [00:31<00:00, 13.17it/s]\n",
      "epoch 07 valid acc 0.806846\n",
      "epoch 08 batch id 0001/3105 loss 0.004811 train acc 1.000000\n",
      "epoch 08 batch id 0201/3105 loss 0.018318 train acc 0.771144\n",
      "epoch 08 batch id 0401/3105 loss 0.002734 train acc 0.812968\n",
      "epoch 08 batch id 0601/3105 loss 1.117530 train acc 0.802274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 08 batch id 0801/3105 loss 0.326475 train acc 0.826259\n",
      "epoch 08 batch id 1001/3105 loss 1.469251 train acc 0.837163\n",
      "epoch 08 batch id 1201/3105 loss 0.003693 train acc 0.842770\n",
      "epoch 08 batch id 1401/3105 loss 0.005497 train acc 0.852605\n",
      "epoch 08 batch id 1601/3105 loss 0.292724 train acc 0.844576\n",
      "epoch 08 batch id 1801/3105 loss 1.847094 train acc 0.842217\n",
      "epoch 08 batch id 2001/3105 loss 0.003793 train acc 0.837331\n",
      "epoch 08 batch id 2201/3105 loss 0.004629 train acc 0.827275\n",
      "epoch 08 batch id 2401/3105 loss 0.004449 train acc 0.823893\n",
      "epoch 08 batch id 2601/3105 loss 0.004800 train acc 0.824298\n",
      "epoch 08 batch id 2801/3105 loss 0.002405 train acc 0.829466\n",
      "epoch 08 batch id 3001/3105 loss 0.016127 train acc 0.828502\n",
      "100%|██████████| 3105/3105 [12:26<00:00,  4.16it/s]\n",
      "epoch 08 train acc 0.831079\n",
      "100%|██████████| 409/409 [00:31<00:00, 13.15it/s]\n",
      "epoch 08 valid acc 0.804808\n",
      "epoch 09 batch id 0001/3105 loss 0.017016 train acc 1.000000\n",
      "epoch 09 batch id 0201/3105 loss 0.009073 train acc 0.838308\n",
      "epoch 09 batch id 0401/3105 loss 0.005730 train acc 0.847049\n",
      "epoch 09 batch id 0601/3105 loss 0.985303 train acc 0.860233\n",
      "epoch 09 batch id 0801/3105 loss 0.006483 train acc 0.873700\n",
      "epoch 09 batch id 1001/3105 loss 0.001970 train acc 0.879121\n",
      "epoch 09 batch id 1201/3105 loss 0.003931 train acc 0.880239\n",
      "epoch 09 batch id 1401/3105 loss 0.003709 train acc 0.884249\n",
      "epoch 09 batch id 1601/3105 loss 0.013603 train acc 0.887362\n",
      "epoch 09 batch id 1801/3105 loss 2.896368 train acc 0.888210\n",
      "epoch 09 batch id 2001/3105 loss 0.004658 train acc 0.877645\n",
      "epoch 09 batch id 2201/3105 loss 0.480237 train acc 0.873088\n",
      "epoch 09 batch id 2401/3105 loss 0.006145 train acc 0.874011\n",
      "epoch 09 batch id 2601/3105 loss 0.006072 train acc 0.870947\n",
      "epoch 09 batch id 2801/3105 loss 0.005059 train acc 0.873974\n",
      "epoch 09 batch id 3001/3105 loss 0.597212 train acc 0.870099\n",
      "100%|██████████| 3105/3105 [12:24<00:00,  4.17it/s]\n",
      "epoch 09 train acc 0.871444\n",
      "100%|██████████| 409/409 [00:31<00:00, 13.18it/s]\n",
      "epoch 09 valid acc 0.810106\n",
      "epoch 10 batch id 0001/3105 loss 0.005373 train acc 1.000000\n",
      "epoch 10 batch id 0201/3105 loss 0.003594 train acc 0.853234\n",
      "epoch 10 batch id 0401/3105 loss 0.256986 train acc 0.831255\n",
      "epoch 10 batch id 0601/3105 loss 0.025017 train acc 0.848863\n",
      "epoch 10 batch id 0801/3105 loss 0.001973 train acc 0.866625\n",
      "epoch 10 batch id 1001/3105 loss 0.674359 train acc 0.869963\n",
      "epoch 10 batch id 1201/3105 loss 0.002797 train acc 0.877186\n",
      "epoch 10 batch id 1401/3105 loss 0.004254 train acc 0.881632\n",
      "epoch 10 batch id 1601/3105 loss 0.829805 train acc 0.884864\n",
      "epoch 10 batch id 1801/3105 loss 2.100239 train acc 0.885619\n",
      "epoch 10 batch id 2001/3105 loss 0.004049 train acc 0.880143\n",
      "epoch 10 batch id 2201/3105 loss 0.003025 train acc 0.880887\n",
      "epoch 10 batch id 2401/3105 loss 0.003706 train acc 0.884215\n",
      "epoch 10 batch id 2601/3105 loss 0.003850 train acc 0.887031\n",
      "epoch 10 batch id 2801/3105 loss 0.005610 train acc 0.889563\n",
      "epoch 10 batch id 3001/3105 loss 0.009431 train acc 0.886094\n",
      "100%|██████████| 3105/3105 [12:25<00:00,  4.16it/s]\n",
      "epoch 10 train acc 0.888030\n",
      "100%|██████████| 409/409 [00:31<00:00, 13.18it/s]\n",
      "epoch 10 valid acc 0.798289\n",
      "epoch 11 batch id 0001/3105 loss 0.005900 train acc 1.000000\n",
      "epoch 11 batch id 0201/3105 loss 0.008785 train acc 0.880597\n",
      "epoch 11 batch id 0401/3105 loss 0.003823 train acc 0.887781\n",
      "epoch 11 batch id 0601/3105 loss 2.599925 train acc 0.895729\n",
      "epoch 11 batch id 0801/3105 loss 0.001591 train acc 0.902206\n",
      "epoch 11 batch id 1001/3105 loss 1.264790 train acc 0.898934\n",
      "epoch 11 batch id 1201/3105 loss 0.003092 train acc 0.899251\n",
      "epoch 11 batch id 1401/3105 loss 0.004017 train acc 0.904116\n",
      "epoch 11 batch id 1601/3105 loss 0.030413 train acc 0.907037\n",
      "epoch 11 batch id 1801/3105 loss 2.040264 train acc 0.905053\n",
      "epoch 11 batch id 2001/3105 loss 0.003228 train acc 0.900383\n",
      "epoch 11 batch id 2201/3105 loss 0.002518 train acc 0.898682\n",
      "epoch 11 batch id 2401/3105 loss 0.004415 train acc 0.901083\n",
      "epoch 11 batch id 2601/3105 loss 0.004074 train acc 0.902730\n",
      "epoch 11 batch id 2801/3105 loss 0.006293 train acc 0.905272\n",
      "epoch 11 batch id 3001/3105 loss 0.008070 train acc 0.903588\n",
      "100%|██████████| 3105/3105 [12:25<00:00,  4.17it/s]\n",
      "epoch 11 train acc 0.904777\n",
      "100%|██████████| 409/409 [00:31<00:00, 13.19it/s]\n",
      "epoch 11 valid acc 0.807661\n",
      "epoch 12 batch id 0001/3105 loss 0.003287 train acc 1.000000\n",
      "epoch 12 batch id 0201/3105 loss 0.008921 train acc 0.877280\n",
      "epoch 12 batch id 0401/3105 loss 0.006911 train acc 0.891937\n",
      "epoch 12 batch id 0601/3105 loss 2.009806 train acc 0.895729\n",
      "epoch 12 batch id 0801/3105 loss 0.000907 train acc 0.902414\n",
      "epoch 12 batch id 1001/3105 loss 0.005358 train acc 0.901765\n",
      "epoch 12 batch id 1201/3105 loss 0.001820 train acc 0.907161\n",
      "epoch 12 batch id 1401/3105 loss 0.006566 train acc 0.911135\n",
      "epoch 12 batch id 1601/3105 loss 0.715628 train acc 0.909119\n",
      "epoch 12 batch id 1801/3105 loss 2.717360 train acc 0.905238\n",
      "epoch 12 batch id 2001/3105 loss 0.002646 train acc 0.901133\n",
      "epoch 12 batch id 2201/3105 loss 0.005345 train acc 0.900651\n",
      "epoch 12 batch id 2401/3105 loss 0.012064 train acc 0.902679\n",
      "epoch 12 batch id 2601/3105 loss 0.002560 train acc 0.901833\n",
      "epoch 12 batch id 2801/3105 loss 0.002732 train acc 0.904736\n",
      "epoch 12 batch id 3001/3105 loss 0.003628 train acc 0.904476\n",
      "100%|██████████| 3105/3105 [12:24<00:00,  4.17it/s]\n",
      "epoch 12 train acc 0.905851\n",
      "100%|██████████| 409/409 [00:31<00:00, 13.16it/s]\n",
      "epoch 12 valid acc 0.814996\n",
      "epoch 13 batch id 0001/3105 loss 0.004729 train acc 1.000000\n",
      "epoch 13 batch id 0201/3105 loss 0.827368 train acc 0.893035\n",
      "epoch 13 batch id 0401/3105 loss 0.004540 train acc 0.903159\n",
      "epoch 13 batch id 0601/3105 loss 2.331468 train acc 0.903771\n",
      "epoch 13 batch id 0801/3105 loss 0.002217 train acc 0.908448\n",
      "epoch 13 batch id 1001/3105 loss 0.003117 train acc 0.912587\n",
      "epoch 13 batch id 1201/3105 loss 0.002345 train acc 0.916042\n",
      "epoch 13 batch id 1401/3105 loss 0.006887 train acc 0.919581\n",
      "epoch 13 batch id 1601/3105 loss 1.037420 train acc 0.922757\n",
      "epoch 13 batch id 1801/3105 loss 2.686453 train acc 0.922080\n",
      "epoch 13 batch id 2001/3105 loss 0.005186 train acc 0.916292\n",
      "epoch 13 batch id 2201/3105 loss 0.004208 train acc 0.915417\n",
      "epoch 13 batch id 2401/3105 loss 0.007542 train acc 0.917326\n",
      "epoch 13 batch id 2601/3105 loss 0.003673 train acc 0.918813\n",
      "epoch 13 batch id 2801/3105 loss 0.001902 train acc 0.920386\n",
      "epoch 13 batch id 3001/3105 loss 0.008177 train acc 0.919582\n",
      "100%|██████████| 3105/3105 [12:22<00:00,  4.18it/s]\n",
      "epoch 13 train acc 0.921041\n",
      "100%|██████████| 409/409 [00:31<00:00, 13.18it/s]\n",
      "epoch 13 valid acc 0.819886\n",
      "epoch 14 batch id 0001/3105 loss 0.003940 train acc 1.000000\n",
      "epoch 14 batch id 0201/3105 loss 0.003909 train acc 0.904643\n",
      "epoch 14 batch id 0401/3105 loss 0.003924 train acc 0.916043\n",
      "epoch 14 batch id 0601/3105 loss 0.029342 train acc 0.912923\n",
      "epoch 14 batch id 0801/3105 loss 0.001358 train acc 0.915522\n",
      "epoch 14 batch id 1001/3105 loss 0.003326 train acc 0.918748\n",
      "epoch 14 batch id 1201/3105 loss 0.002086 train acc 0.923536\n",
      "epoch 14 batch id 1401/3105 loss 0.003353 train acc 0.925410\n",
      "epoch 14 batch id 1601/3105 loss 0.015682 train acc 0.926400\n",
      "epoch 14 batch id 1801/3105 loss 2.582829 train acc 0.927540\n",
      "epoch 14 batch id 2001/3105 loss 0.003096 train acc 0.923038\n",
      "epoch 14 batch id 2201/3105 loss 0.002319 train acc 0.920188\n",
      "epoch 14 batch id 2401/3105 loss 0.008959 train acc 0.922463\n",
      "epoch 14 batch id 2601/3105 loss 0.003013 train acc 0.924388\n",
      "epoch 14 batch id 2801/3105 loss 0.002745 train acc 0.925860\n",
      "epoch 14 batch id 3001/3105 loss 0.001896 train acc 0.924303\n",
      "100%|██████████| 3105/3105 [12:25<00:00,  4.17it/s]\n",
      "epoch 14 train acc 0.925335\n",
      "100%|██████████| 409/409 [00:30<00:00, 13.21it/s]\n",
      "epoch 14 valid acc 0.817441\n",
      "epoch 15 batch id 0001/3105 loss 0.003077 train acc 1.000000\n",
      "epoch 15 batch id 0201/3105 loss 0.003520 train acc 0.902156\n",
      "epoch 15 batch id 0401/3105 loss 0.003055 train acc 0.919368\n",
      "epoch 15 batch id 0601/3105 loss 1.117010 train acc 0.919578\n",
      "epoch 15 batch id 0801/3105 loss 0.000871 train acc 0.921764\n",
      "epoch 15 batch id 1001/3105 loss 0.001148 train acc 0.923743\n",
      "epoch 15 batch id 1201/3105 loss 0.001604 train acc 0.925756\n",
      "epoch 15 batch id 1401/3105 loss 0.004101 train acc 0.927909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 15 batch id 1601/3105 loss 0.008915 train acc 0.930460\n",
      "epoch 15 batch id 1801/3105 loss 2.327178 train acc 0.931520\n",
      "epoch 15 batch id 2001/3105 loss 0.002596 train acc 0.927620\n",
      "epoch 15 batch id 2201/3105 loss 0.002473 train acc 0.927079\n",
      "epoch 15 batch id 2401/3105 loss 0.008666 train acc 0.928710\n",
      "epoch 15 batch id 2601/3105 loss 0.002198 train acc 0.930027\n",
      "epoch 15 batch id 2801/3105 loss 0.002592 train acc 0.931989\n",
      "epoch 15 batch id 3001/3105 loss 0.002672 train acc 0.931134\n",
      "100%|██████████| 3105/3105 [12:23<00:00,  4.18it/s]\n",
      "epoch 15 train acc 0.932206\n",
      "100%|██████████| 409/409 [00:31<00:00, 13.17it/s]\n",
      "epoch 15 valid acc 0.818663\n",
      "epoch 16 batch id 0001/3105 loss 0.002671 train acc 1.000000\n",
      "epoch 16 batch id 0201/3105 loss 0.001205 train acc 0.912106\n",
      "epoch 16 batch id 0401/3105 loss 0.002566 train acc 0.923525\n",
      "epoch 16 batch id 0601/3105 loss 0.005648 train acc 0.924847\n",
      "epoch 16 batch id 0801/3105 loss 0.001227 train acc 0.930087\n",
      "epoch 16 batch id 1001/3105 loss 0.001203 train acc 0.932567\n",
      "epoch 16 batch id 1201/3105 loss 0.001681 train acc 0.936581\n",
      "epoch 16 batch id 1401/3105 loss 0.002386 train acc 0.938734\n",
      "epoch 16 batch id 1601/3105 loss 0.008571 train acc 0.940766\n",
      "epoch 16 batch id 1801/3105 loss 2.295257 train acc 0.942347\n",
      "epoch 16 batch id 2001/3105 loss 0.003011 train acc 0.939030\n",
      "epoch 16 batch id 2201/3105 loss 0.001863 train acc 0.937604\n",
      "epoch 16 batch id 2401/3105 loss 0.006267 train acc 0.939053\n",
      "epoch 16 batch id 2601/3105 loss 0.002986 train acc 0.940087\n",
      "epoch 16 batch id 2801/3105 loss 0.002442 train acc 0.941747\n",
      "epoch 16 batch id 3001/3105 loss 0.002586 train acc 0.940798\n",
      "100%|██████████| 3105/3105 [12:22<00:00,  4.18it/s]\n",
      "epoch 16 train acc 0.941546\n",
      "100%|██████████| 409/409 [00:31<00:00, 13.18it/s]\n",
      "epoch 16 valid acc 0.819886\n",
      "epoch 17 batch id 0001/3105 loss 0.002739 train acc 1.000000\n",
      "epoch 17 batch id 0201/3105 loss 0.002206 train acc 0.922886\n",
      "epoch 17 batch id 0401/3105 loss 0.001683 train acc 0.934331\n",
      "epoch 17 batch id 0601/3105 loss 0.004050 train acc 0.934554\n",
      "epoch 17 batch id 0801/3105 loss 0.001071 train acc 0.939035\n",
      "epoch 17 batch id 1001/3105 loss 0.000654 train acc 0.940892\n",
      "epoch 17 batch id 1201/3105 loss 0.002112 train acc 0.943936\n",
      "epoch 17 batch id 1401/3105 loss 0.002875 train acc 0.945872\n",
      "epoch 17 batch id 1601/3105 loss 0.008905 train acc 0.947012\n",
      "epoch 17 batch id 1801/3105 loss 2.142404 train acc 0.947992\n",
      "epoch 17 batch id 2001/3105 loss 0.002002 train acc 0.944444\n",
      "epoch 17 batch id 2201/3105 loss 0.002381 train acc 0.943283\n",
      "epoch 17 batch id 2401/3105 loss 0.005711 train acc 0.944606\n",
      "epoch 17 batch id 2601/3105 loss 0.002822 train acc 0.945277\n",
      "epoch 17 batch id 2801/3105 loss 0.001966 train acc 0.947043\n",
      "epoch 17 batch id 3001/3105 loss 0.003166 train acc 0.946018\n",
      "100%|██████████| 3105/3105 [12:23<00:00,  4.18it/s]\n",
      "epoch 17 train acc 0.946592\n",
      "100%|██████████| 409/409 [00:31<00:00, 13.18it/s]\n",
      "epoch 17 valid acc 0.828036\n",
      "epoch 18 batch id 0001/3105 loss 0.002786 train acc 1.000000\n",
      "epoch 18 batch id 0201/3105 loss 0.002365 train acc 0.931177\n",
      "epoch 18 batch id 0401/3105 loss 0.001503 train acc 0.938903\n",
      "epoch 18 batch id 0601/3105 loss 0.005783 train acc 0.938713\n",
      "epoch 18 batch id 0801/3105 loss 0.001230 train acc 0.943196\n",
      "epoch 18 batch id 1001/3105 loss 0.000543 train acc 0.945721\n",
      "epoch 18 batch id 1201/3105 loss 0.002554 train acc 0.948515\n",
      "epoch 18 batch id 1401/3105 loss 0.002859 train acc 0.949679\n",
      "epoch 18 batch id 1601/3105 loss 0.007197 train acc 0.950760\n",
      "epoch 18 batch id 1801/3105 loss 1.999408 train acc 0.951416\n",
      "epoch 18 batch id 2001/3105 loss 0.001271 train acc 0.948026\n",
      "epoch 18 batch id 2201/3105 loss 0.002322 train acc 0.946994\n",
      "epoch 18 batch id 2401/3105 loss 0.004117 train acc 0.948077\n",
      "epoch 18 batch id 2601/3105 loss 0.002691 train acc 0.948994\n",
      "epoch 18 batch id 2801/3105 loss 0.002402 train acc 0.950494\n",
      "epoch 18 batch id 3001/3105 loss 0.002720 train acc 0.949572\n",
      "100%|██████████| 3105/3105 [12:24<00:00,  4.17it/s]\n",
      "epoch 18 train acc 0.950188\n",
      "100%|██████████| 409/409 [00:31<00:00, 13.18it/s]\n",
      "epoch 18 valid acc 0.825183\n",
      "epoch 19 batch id 0001/3105 loss 0.002746 train acc 1.000000\n",
      "epoch 19 batch id 0201/3105 loss 0.002177 train acc 0.932836\n",
      "epoch 19 batch id 0401/3105 loss 0.001175 train acc 0.941397\n",
      "epoch 19 batch id 0601/3105 loss 0.003796 train acc 0.942596\n",
      "epoch 19 batch id 0801/3105 loss 0.001291 train acc 0.947357\n",
      "epoch 19 batch id 1001/3105 loss 0.001610 train acc 0.949883\n",
      "epoch 19 batch id 1201/3105 loss 0.001896 train acc 0.952262\n",
      "epoch 19 batch id 1401/3105 loss 0.003223 train acc 0.953248\n",
      "epoch 19 batch id 1601/3105 loss 0.006449 train acc 0.953883\n",
      "epoch 19 batch id 1801/3105 loss 2.158572 train acc 0.954655\n",
      "epoch 19 batch id 2001/3105 loss 0.001546 train acc 0.951774\n",
      "epoch 19 batch id 2201/3105 loss 0.002292 train acc 0.950780\n",
      "epoch 19 batch id 2401/3105 loss 0.005152 train acc 0.951895\n",
      "epoch 19 batch id 2601/3105 loss 0.002606 train acc 0.952646\n",
      "epoch 19 batch id 2801/3105 loss 0.001993 train acc 0.954005\n",
      "epoch 19 batch id 3001/3105 loss 0.002608 train acc 0.952794\n",
      "100%|██████████| 3105/3105 [12:19<00:00,  4.20it/s]\n",
      "epoch 19 train acc 0.953301\n",
      "100%|██████████| 409/409 [00:31<00:00, 13.18it/s]\n",
      "epoch 19 valid acc 0.829258\n",
      "epoch 20 batch id 0001/3105 loss 0.001963 train acc 1.000000\n",
      "epoch 20 batch id 0201/3105 loss 0.001482 train acc 0.941128\n",
      "epoch 20 batch id 0401/3105 loss 0.001174 train acc 0.946384\n",
      "epoch 20 batch id 0601/3105 loss 0.003165 train acc 0.946755\n",
      "epoch 20 batch id 0801/3105 loss 0.000748 train acc 0.949438\n",
      "epoch 20 batch id 1001/3105 loss 0.000665 train acc 0.951715\n",
      "epoch 20 batch id 1201/3105 loss 0.001688 train acc 0.953372\n",
      "epoch 20 batch id 1401/3105 loss 0.002096 train acc 0.954437\n",
      "epoch 20 batch id 1601/3105 loss 0.007260 train acc 0.955236\n",
      "epoch 20 batch id 1801/3105 loss 2.185992 train acc 0.955950\n",
      "epoch 20 batch id 2001/3105 loss 0.002111 train acc 0.952940\n",
      "epoch 20 batch id 2201/3105 loss 0.001614 train acc 0.952067\n",
      "epoch 20 batch id 2401/3105 loss 0.005945 train acc 0.952797\n",
      "epoch 20 batch id 2601/3105 loss 0.003420 train acc 0.953608\n",
      "epoch 20 batch id 2801/3105 loss 0.001893 train acc 0.954897\n",
      "epoch 20 batch id 3001/3105 loss 0.002979 train acc 0.953627\n",
      "100%|██████████| 3105/3105 [12:19<00:00,  4.20it/s]\n",
      "epoch 20 train acc 0.954053\n",
      "100%|██████████| 409/409 [00:31<00:00, 13.17it/s]\n",
      "epoch 20 valid acc 0.828443\n",
      "epoch 21 batch id 0001/3105 loss 0.002093 train acc 1.000000\n",
      "epoch 21 batch id 0201/3105 loss 0.002075 train acc 0.941957\n",
      "epoch 21 batch id 0401/3105 loss 0.001994 train acc 0.947631\n",
      "epoch 21 batch id 0601/3105 loss 0.004484 train acc 0.948974\n",
      "epoch 21 batch id 0801/3105 loss 0.000992 train acc 0.951935\n",
      "epoch 21 batch id 1001/3105 loss 0.000718 train acc 0.954212\n",
      "epoch 21 batch id 1201/3105 loss 0.001301 train acc 0.955454\n",
      "epoch 21 batch id 1401/3105 loss 0.002214 train acc 0.956460\n",
      "epoch 21 batch id 1601/3105 loss 0.007187 train acc 0.956798\n",
      "epoch 21 batch id 1801/3105 loss 2.064327 train acc 0.957339\n",
      "epoch 21 batch id 2001/3105 loss 0.001115 train acc 0.954106\n",
      "epoch 21 batch id 2201/3105 loss 0.001523 train acc 0.953203\n",
      "epoch 21 batch id 2401/3105 loss 0.003881 train acc 0.953839\n",
      "epoch 21 batch id 2601/3105 loss 0.002569 train acc 0.954441\n",
      "epoch 21 batch id 2801/3105 loss 0.002773 train acc 0.955492\n",
      "epoch 21 batch id 3001/3105 loss 0.002119 train acc 0.954293\n",
      "100%|██████████| 3105/3105 [12:20<00:00,  4.19it/s]\n",
      "epoch 21 train acc 0.954697\n",
      "100%|██████████| 409/409 [00:30<00:00, 13.20it/s]\n",
      "epoch 21 valid acc 0.828851\n",
      "epoch 22 batch id 0001/3105 loss 0.002340 train acc 1.000000\n",
      "epoch 22 batch id 0201/3105 loss 0.001894 train acc 0.940299\n",
      "epoch 22 batch id 0401/3105 loss 0.001407 train acc 0.946384\n",
      "epoch 22 batch id 0601/3105 loss 0.003277 train acc 0.950083\n",
      "epoch 22 batch id 0801/3105 loss 0.001050 train acc 0.952559\n",
      "epoch 22 batch id 1001/3105 loss 0.000732 train acc 0.953213\n",
      "epoch 22 batch id 1201/3105 loss 0.001523 train acc 0.955176\n",
      "epoch 22 batch id 1401/3105 loss 0.003019 train acc 0.955865\n",
      "epoch 22 batch id 1601/3105 loss 0.006952 train acc 0.956173\n",
      "epoch 22 batch id 1801/3105 loss 1.899926 train acc 0.956968\n",
      "epoch 22 batch id 2001/3105 loss 0.001621 train acc 0.953940\n",
      "epoch 22 batch id 2201/3105 loss 0.002108 train acc 0.953203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 22 batch id 2401/3105 loss 0.002976 train acc 0.954047\n",
      "epoch 22 batch id 2601/3105 loss 0.002528 train acc 0.954505\n",
      "epoch 22 batch id 2801/3105 loss 0.003108 train acc 0.955730\n",
      "epoch 22 batch id 3001/3105 loss 0.002361 train acc 0.954682\n",
      "100%|██████████| 3105/3105 [12:21<00:00,  4.19it/s]\n",
      "epoch 22 train acc 0.955072\n",
      "100%|██████████| 409/409 [00:31<00:00, 13.18it/s]\n",
      "epoch 22 valid acc 0.829666\n",
      "epoch 23 batch id 0001/3105 loss 0.003315 train acc 1.000000\n",
      "epoch 23 batch id 0201/3105 loss 0.002285 train acc 0.940299\n",
      "epoch 23 batch id 0401/3105 loss 0.001366 train acc 0.947215\n",
      "epoch 23 batch id 0601/3105 loss 0.004089 train acc 0.948697\n",
      "epoch 23 batch id 0801/3105 loss 0.001115 train acc 0.952143\n",
      "epoch 23 batch id 1001/3105 loss 0.000732 train acc 0.953713\n",
      "epoch 23 batch id 1201/3105 loss 0.001396 train acc 0.955037\n",
      "epoch 23 batch id 1401/3105 loss 0.002381 train acc 0.955865\n",
      "epoch 23 batch id 1601/3105 loss 0.007003 train acc 0.956277\n",
      "epoch 23 batch id 1801/3105 loss 2.062234 train acc 0.956968\n",
      "epoch 23 batch id 2001/3105 loss 0.001532 train acc 0.953940\n",
      "epoch 23 batch id 2201/3105 loss 0.001642 train acc 0.952749\n",
      "epoch 23 batch id 2401/3105 loss 0.002899 train acc 0.953630\n",
      "epoch 23 batch id 2601/3105 loss 0.002952 train acc 0.954377\n",
      "epoch 23 batch id 2801/3105 loss 0.001395 train acc 0.955492\n",
      "epoch 23 batch id 3001/3105 loss 0.002591 train acc 0.954626\n",
      "100%|██████████| 3105/3105 [12:23<00:00,  4.17it/s]\n",
      "epoch 23 train acc 0.955072\n",
      "100%|██████████| 409/409 [00:31<00:00, 13.17it/s]\n",
      "epoch 23 valid acc 0.825183\n",
      "epoch 24 batch id 0001/3105 loss 0.002335 train acc 1.000000\n",
      "epoch 24 batch id 0201/3105 loss 0.001770 train acc 0.941128\n",
      "epoch 24 batch id 0401/3105 loss 0.001520 train acc 0.946800\n",
      "epoch 24 batch id 0601/3105 loss 0.002196 train acc 0.948697\n",
      "epoch 24 batch id 0801/3105 loss 0.000811 train acc 0.952143\n",
      "epoch 24 batch id 1001/3105 loss 0.000649 train acc 0.953380\n",
      "epoch 24 batch id 1201/3105 loss 0.001307 train acc 0.955037\n",
      "epoch 24 batch id 1401/3105 loss 0.002095 train acc 0.956103\n",
      "epoch 24 batch id 1601/3105 loss 0.005668 train acc 0.956902\n",
      "epoch 24 batch id 1801/3105 loss 1.727208 train acc 0.957986\n",
      "epoch 24 batch id 2001/3105 loss 0.001349 train acc 0.954689\n",
      "epoch 24 batch id 2201/3105 loss 0.001425 train acc 0.953506\n",
      "epoch 24 batch id 2401/3105 loss 0.003398 train acc 0.954325\n",
      "epoch 24 batch id 2601/3105 loss 0.003114 train acc 0.954889\n",
      "epoch 24 batch id 2801/3105 loss 0.002589 train acc 0.956266\n",
      "epoch 24 batch id 3001/3105 loss 0.002217 train acc 0.955071\n",
      "100%|██████████| 3105/3105 [12:21<00:00,  4.18it/s]\n",
      "epoch 24 train acc 0.955556\n",
      "100%|██████████| 409/409 [00:31<00:00, 13.18it/s]\n",
      "epoch 24 valid acc 0.828443\n",
      "epoch 25 batch id 0001/3105 loss 0.001986 train acc 1.000000\n",
      "epoch 25 batch id 0201/3105 loss 0.001965 train acc 0.941957\n",
      "epoch 25 batch id 0401/3105 loss 0.001631 train acc 0.948047\n",
      "epoch 25 batch id 0601/3105 loss 0.003705 train acc 0.951747\n",
      "epoch 25 batch id 0801/3105 loss 0.000962 train acc 0.954224\n",
      "epoch 25 batch id 1001/3105 loss 0.000522 train acc 0.955711\n",
      "epoch 25 batch id 1201/3105 loss 0.001857 train acc 0.957119\n",
      "epoch 25 batch id 1401/3105 loss 0.003010 train acc 0.957768\n",
      "epoch 25 batch id 1601/3105 loss 0.008067 train acc 0.958151\n",
      "epoch 25 batch id 1801/3105 loss 2.076035 train acc 0.958727\n",
      "epoch 25 batch id 2001/3105 loss 0.001773 train acc 0.955272\n",
      "epoch 25 batch id 2201/3105 loss 0.001335 train acc 0.954339\n",
      "epoch 25 batch id 2401/3105 loss 0.006165 train acc 0.953214\n",
      "epoch 25 batch id 2601/3105 loss 0.002469 train acc 0.947969\n",
      "epoch 25 batch id 2801/3105 loss 0.002298 train acc 0.944187\n",
      "epoch 25 batch id 3001/3105 loss 0.002398 train acc 0.941575\n",
      "100%|██████████| 3105/3105 [12:22<00:00,  4.18it/s]\n",
      "epoch 25 train acc 0.941116\n",
      "100%|██████████| 409/409 [00:31<00:00, 13.16it/s]\n",
      "epoch 25 valid acc 0.765281\n"
     ]
    }
   ],
   "source": [
    "pickle.dump(label_w2i, open('ver_3.w2i', 'wb'))\n",
    "result = []\n",
    "\n",
    "n_batch = len(train_dataloader)\n",
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    valid_acc = 0.0\n",
    "    model.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(train_dataloader, file=sys.stdout)):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            tqdm.write(\"epoch %02d batch id %04d/%d loss %f train acc %f\" %(e+1, batch_id+1, n_batch, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "    train_acc /= batch_id + 1\n",
    "    print(\"epoch %02d train acc %f\" %(e+1, train_acc))\n",
    "    model.eval()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(valid_dataloader, file=sys.stdout)):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        valid_acc += calc_accuracy(out, label)\n",
    "    valid_acc /=  batch_id+1\n",
    "    print(\"epoch %02d valid acc %f\" %(e+1, valid_acc))\n",
    "    torch.save(model, 'ver_3_%02d_%.04f_%.04f.model' %(e+1, train_acc, valid_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score, precision_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('ver_2_16_0.9530_0.8333.model')\n",
    "label_w2i = pickle.load(open('ver_2.w2i', 'rb'))\n",
    "label_i2w = {label_w2i[l]: l for l in label_w2i}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred(X):\n",
    "    _, indices = torch.max(X, 1)\n",
    "    return indices.cpu().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 380/380 [00:28<00:00, 13.38it/s]\n"
     ]
    }
   ],
   "source": [
    "valid_acc = 0\n",
    "test_pred = []\n",
    "model.eval()\n",
    "for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(test_dataloader, file=sys.stdout)):\n",
    "    token_ids = token_ids.long().to(device)\n",
    "    segment_ids = segment_ids.long().to(device)\n",
    "    valid_length= valid_length\n",
    "    label = label.long().to(device)\n",
    "    out = model(token_ids, valid_length, segment_ids)\n",
    "    test_pred.append(get_pred(out))\n",
    "test_pred = np.concatenate(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_true = np.array([d[2] for d in test_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_acc = (test_pred == test_true).sum() / len(test_true)\n",
    "test_precision = [precision_score(test_true, test_pred, labels=[i], average='macro') for i in label_i2w]\n",
    "test_recall = [recall_score(test_true, test_pred, labels=[i], average='macro') for i in label_i2w]\n",
    "test_f1 = [f1_score(test_true, test_pred, labels=[i], average='macro') for i in label_i2w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Label | Precision | Recall | F1 |\n",
      "|-------|-----------|--------|----|\n",
      "| 가족 | 0.7439 | 0.5755 | 0.6489 |\n",
      "| 건강/다이어트 | 0.7698 | 0.8818 | 0.8220 |\n",
      "| 계절/날씨 | 0.9200 | 0.8647 | 0.8915 |\n",
      "| 꿈(목표) | 0.6134 | 0.7766 | 0.6854 |\n",
      "| 먹거리 | 0.9209 | 0.8996 | 0.9102 |\n",
      "| 반려동물 | 0.9590 | 0.9832 | 0.9710 |\n",
      "| 방송/연예 | 0.9101 | 0.6207 | 0.7380 |\n",
      "| 선물 | 0.9224 | 0.9068 | 0.9145 |\n",
      "| 성격 | 0.9256 | 0.8960 | 0.9106 |\n",
      "| 스포츠/레저 | 0.8471 | 0.8110 | 0.8287 |\n",
      "| 아르바이트 | 0.7403 | 0.9828 | 0.8444 |\n",
      "| 여행지(국내/해외) | 0.8508 | 0.9277 | 0.8876 |\n",
      "| 연애/결혼 | 0.6902 | 0.9137 | 0.7864 |\n",
      "| 영화 | 0.8424 | 0.9553 | 0.8953 |\n",
      "| 회사/학교 | 0.8205 | 0.6809 | 0.7442 |\n",
      "| Total | 0.8318 | 0.8451 | 0.8319 |\n",
      "Test Accuracy : 0.8362\n"
     ]
    }
   ],
   "source": [
    "print('| Label | Precision | Recall | F1 |')\n",
    "print('|-------|-----------|--------|----|')\n",
    "for i, (precision, recall, f1) in enumerate(zip(test_precision, test_recall, test_f1)):\n",
    "    print('| %s | %.4f | %.4f | %.4f |' %(label_i2w[i], precision, recall, f1))\n",
    "print('| %s | %.4f | %.4f | %.4f |' %(\n",
    "    'Total', \n",
    "    precision_score(test_true, test_pred, average='macro'), \n",
    "    recall_score(test_true, test_pred, average='macro'), \n",
    "    f1_score(test_true, test_pred, average='macro'), \n",
    "))\n",
    "\n",
    "print('Test Accuracy : %.4f' %test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13, 13, 13, ...,  6,  6,  6])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 91.12it/s]\n"
     ]
    }
   ],
   "source": [
    "raw_data = [\n",
    "    ('혹시 마블 좋아하세요?', '네. 최근에 스파이더맨 봤어요.'),\n",
    "    ('점심 뭐 드셨어요?', '근처에서 마라탕 먹었어요.')\n",
    "]\n",
    "data = BERTDataset([d + (0,) for d in raw_data], 0, 1, 2, tok, max_len, True, False)\n",
    "dataloader = DataLoader(data, batch_size=batch_size, num_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/svclaw2000/Dialog-Classification-Using-KoBERT/venv_kcc/lib/python3.7/site-packages/ipykernel_launcher.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4885577bcf5f4ea9ae9753fd0c1ddd06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "outputs = []\n",
    "for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(dataloader)):\n",
    "    token_ids = token_ids.long().to(device)\n",
    "    segment_ids = segment_ids.long().to(device)\n",
    "    valid_length = valid_length\n",
    "    label = label.long().to(device)\n",
    "    out = model(token_ids, valid_length, segment_ids)\n",
    "    outputs.append(out.cpu().detach().numpy())\n",
    "\n",
    "result = np.concatenate(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.6374834 , -1.3215848 ,  0.12254745, -2.5353487 , -0.89550805,\n",
       "        -1.4683422 ,  1.9166542 , -2.0298762 , -1.7553192 ,  1.1083449 ,\n",
       "        -1.3344933 , -0.5012208 ,  0.7741338 ,  8.988237  , -0.67473793],\n",
       "       [ 0.14410633,  0.48958924, -0.43005556, -1.36853   , 10.405009  ,\n",
       "        -1.5924621 , -1.6080155 ,  0.31998512, -2.137295  , -0.46647248,\n",
       "        -0.10372277,  2.2267296 , -1.6280773 , -1.0927978 , -1.1962178 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_length = list(map(lambda x: len(x[2] + x[3]), raw_train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18082"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(full_length)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KCC",
   "language": "python",
   "name": "venv_kcc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
