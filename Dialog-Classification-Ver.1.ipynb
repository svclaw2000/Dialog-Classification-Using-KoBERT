{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os, sys\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sent):\n",
    "    sent = sent.strip()\n",
    "    sent = re.sub('{laughing}|{clearing}|{singing}|{applauding}', '', sent)\n",
    "    sent = re.sub('[(][(].*?[)][)]|-.*?-', '', sent)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_main_topic = lambda x: x.split(' > ')[0]\n",
    "\n",
    "def load_data():\n",
    "    file_list = [f_name for f_name in os.listdir('data') if f_name[-5:] == '.json']\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(file_list)\n",
    "    \n",
    "    valid_ratio = 0.1\n",
    "    test_ratio = 0.1\n",
    "    valid_size = int(len(file_list) * valid_ratio)\n",
    "    test_size = int(len(file_list) * test_ratio)\n",
    "    valid_list = file_list[-(valid_size + test_size):-test_size]\n",
    "    test_list = file_list[-test_size:]\n",
    "    \n",
    "    train_data = []\n",
    "    valid_data = []\n",
    "    test_data = []\n",
    "    for f_name in tqdm(file_list):\n",
    "        with open('data/%s' %f_name, 'r') as f:\n",
    "            data = json.loads(f.read())['document'][0]\n",
    "            metadata = data['metadata']\n",
    "            utterance = data['utterance']\n",
    "            \n",
    "            topic = get_main_topic(metadata['topic'])\n",
    "            if topic[:4] == 'NWRW':\n",
    "                continue\n",
    "\n",
    "            last_speaker = None\n",
    "            seg1 = seg2 = ''\n",
    "            for u in utterance:\n",
    "                if last_speaker is None:\n",
    "                    last_speaker = u['speaker_id']\n",
    "                    seg2 = u['form']\n",
    "                elif last_speaker == u['speaker_id']:\n",
    "                    seg2 += ' ' + u['original_form']\n",
    "                else:\n",
    "                    if seg1 and seg2:\n",
    "                        if f_name in valid_list:\n",
    "                            valid_data.append([f_name, topic, clean_sentence(seg1), clean_sentence(seg2)])\n",
    "                        elif f_name in test_list:\n",
    "                            test_data.append([f_name, topic, clean_sentence(seg1), clean_sentence(seg2)])\n",
    "                        else:\n",
    "                            train_data.append([f_name, topic, clean_sentence(seg1), clean_sentence(seg2)])\n",
    "                    last_speaker = u['speaker_id']\n",
    "                    seg1 = seg2\n",
    "                    seg2 = u['original_form']\n",
    "            if seg1 and seg2:\n",
    "                if f_name in valid_list:\n",
    "                    valid_data.append([f_name, topic, clean_sentence(seg1), clean_sentence(seg2)])\n",
    "                elif f_name in test_list:\n",
    "                    test_data.append([f_name, topic, clean_sentence(seg1), clean_sentence(seg2)])\n",
    "                else:\n",
    "                    train_data.append([f_name, topic, clean_sentence(seg1), clean_sentence(seg2)])\n",
    "    return np.array(train_data), np.array(valid_data), np.array(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n",
      "using cached model\n",
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "bertmodel, vocab = get_pytorch_kobert_model()\n",
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting parameters\n",
    "pre_max_len = 4096\n",
    "max_len = 512\n",
    "batch_size = 6\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 20\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate =  5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.indices([pre_max_len])[0]\n",
    "def pad_transform(transform, sent1, sent2, max_len):\n",
    "    tokens, valid_len, segs = transform([sent1, sent2])\n",
    "    if valid_len < max_len:\n",
    "        return tokens[:max_len], np.array((tokens != 1).sum(), dtype='int32'), segs[:max_len]\n",
    "    \n",
    "    idx2 = indices[segs == 1][:-1]\n",
    "    idx1 = indices[:idx2[0]][1:-1]\n",
    "    if len(idx1) < len(idx2) and (len(idx1) * 2 + 3) < max_len:\n",
    "        idx2 = idx2[:(max_len - 3 - len(idx1))]\n",
    "    elif len(idx2) < len(idx1) and (len(idx2) * 2 + 3) < max_len:\n",
    "        idx1 = idx1[-(max_len - 3 - len(idx2)):]\n",
    "    else:\n",
    "        if len(idx1) < len(idx2):\n",
    "            idx1, idx2 = idx1[-((max_len - 3) // 2):], idx2[:((max_len - 3) // 2 + 1)]\n",
    "        else:\n",
    "            idx1, idx2 = idx1[-((max_len - 3) // 2 + 1):], idx2[:((max_len - 3) // 2)]\n",
    "    return (np.concatenate([[2], tokens[idx1], [3], tokens[idx2],[3]]), \n",
    "             np.array(max_len, dtype='int32'), \n",
    "             np.array([0] * (len(idx1) + 2) + [1] * (len(idx2) + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx1, sent_idx2, label_idx, bert_tokenizer, max_len,\n",
    "                 pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=pre_max_len, pad=pad, pair=pair)\n",
    "\n",
    "        self.sentences = [pad_transform(transform, d[sent_idx1], d[sent_idx2], max_len) for d in tqdm(dataset)]\n",
    "#         self.sentences = [transform([d[sent_idx1], d[sent_idx2]]) for d in tqdm(dataset)]\n",
    "        self.labels = [np.int32(d[label_idx]) for d in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=2,\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        ret = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        pooler = ret[1]\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_to_list(row):\n",
    "    return str(row[2]), str(row[3]), label_w2i[row[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2232/2232 [00:02<00:00, 1046.09it/s]\n"
     ]
    }
   ],
   "source": [
    "raw_train_dataset, raw_valid_dataset, raw_test_dataset = load_data()\n",
    "label_i2w = np.unique(raw_train_dataset[:, 1]).tolist()\n",
    "label_w2i = {w: i for i, w in enumerate(label_i2w)}\n",
    "train_dataset = [row_to_list(r) for r in raw_train_dataset]\n",
    "valid_dataset = [row_to_list(r) for r in raw_valid_dataset]\n",
    "test_dataset = [row_to_list(r) for r in raw_test_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18834/18834 [00:21<00:00, 870.52it/s] \n",
      "100%|██████████| 2270/2270 [00:02<00:00, 862.15it/s] \n",
      "100%|██████████| 2253/2253 [00:02<00:00, 862.33it/s]\n"
     ]
    }
   ],
   "source": [
    "data_train = BERTDataset(train_dataset, 0, 1, 2, tok, max_len, True, True)\n",
    "data_valid = BERTDataset(valid_dataset, 0, 1, 2, tok, max_len, True, True)\n",
    "data_test = BERTDataset(test_dataset, 0, 1, 2, tok, max_len, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)\n",
    "valid_dataloader = torch.utils.data.DataLoader(data_valid, batch_size=batch_size, num_workers=5)\n",
    "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERTClassifier(bertmodel, dr_rate=0.5, num_classes=len(label_i2w)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "# loss_fn = nn.CrossEntropyLoss(weight=weight)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 01 batch id 0001/3139 loss 2.776338 train acc 0.000000\n",
      "epoch 01 batch id 0201/3139 loss 2.672711 train acc 0.087894\n",
      "epoch 01 batch id 0401/3139 loss 2.796778 train acc 0.081879\n",
      "epoch 01 batch id 0601/3139 loss 2.769908 train acc 0.078203\n",
      "epoch 01 batch id 0801/3139 loss 2.996637 train acc 0.082813\n",
      "epoch 01 batch id 1001/3139 loss 2.425073 train acc 0.089910\n",
      "epoch 01 batch id 1201/3139 loss 2.491398 train acc 0.092978\n",
      "epoch 01 batch id 1401/3139 loss 2.903411 train acc 0.117297\n",
      "epoch 01 batch id 1601/3139 loss 1.749927 train acc 0.164689\n",
      " 52%|█████▏    | 1636/3139 [06:36<06:09,  4.07it/s]"
     ]
    }
   ],
   "source": [
    "pickle.dump(label_w2i, open('ver_1.w2i', 'wb'))\n",
    "result = []\n",
    "\n",
    "n_batch = len(train_dataloader)\n",
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    valid_acc = 0.0\n",
    "    model.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(train_dataloader, file=sys.stdout)):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            tqdm.write(\"epoch %02d batch id %04d/%d loss %f train acc %f\" %(e+1, batch_id+1, n_batch, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "    train_acc /= batch_id + 1\n",
    "    print(\"epoch %02d train acc %f\" %(e+1, train_acc))\n",
    "    model.eval()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(valid_dataloader):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        valid_acc += calc_accuracy(out, label)\n",
    "    valid_acc /=  batch_id+1\n",
    "    print(\"epoch %02d valid acc %f\" %(e+1, valid_acc))\n",
    "    torch.save(model, 'ver_1_%02d_%.04f_%.04f.model' %(e+1, train_acc, valid_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm._instances.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('ver.1.model')\n",
    "label_w2i = pickle.load(open('ver.1.w2i', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 91.12it/s]\n"
     ]
    }
   ],
   "source": [
    "raw_data = [\n",
    "    ('혹시 마블 좋아하세요?', '네. 최근에 스파이더맨 봤어요.'),\n",
    "    ('점심 뭐 드셨어요?', '근처에서 마라탕 먹었어요.')\n",
    "]\n",
    "data = BERTDataset([d + (0,) for d in raw_data], 0, 1, 2, tok, max_len, True, False)\n",
    "dataloader = DataLoader(data, batch_size=batch_size, num_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/svclaw2000/Dialog-Classification-Using-KoBERT/venv_kcc/lib/python3.7/site-packages/ipykernel_launcher.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4885577bcf5f4ea9ae9753fd0c1ddd06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "outputs = []\n",
    "for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(dataloader)):\n",
    "    token_ids = token_ids.long().to(device)\n",
    "    segment_ids = segment_ids.long().to(device)\n",
    "    valid_length = valid_length\n",
    "    label = label.long().to(device)\n",
    "    out = model(token_ids, valid_length, segment_ids)\n",
    "    outputs.append(out.cpu().detach().numpy())\n",
    "\n",
    "result = np.concatenate(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.6374834 , -1.3215848 ,  0.12254745, -2.5353487 , -0.89550805,\n",
       "        -1.4683422 ,  1.9166542 , -2.0298762 , -1.7553192 ,  1.1083449 ,\n",
       "        -1.3344933 , -0.5012208 ,  0.7741338 ,  8.988237  , -0.67473793],\n",
       "       [ 0.14410633,  0.48958924, -0.43005556, -1.36853   , 10.405009  ,\n",
       "        -1.5924621 , -1.6080155 ,  0.31998512, -2.137295  , -0.46647248,\n",
       "        -0.10372277,  2.2267296 , -1.6280773 , -1.0927978 , -1.1962178 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_length = list(map(lambda x: len(x[2] + x[3]), raw_train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18082"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(full_length)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KCC",
   "language": "python",
   "name": "venv_kcc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
